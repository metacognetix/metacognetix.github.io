## Getting started with Big Data and Apache Spark
![Apache SPark logo](slideassets/spark-white.png)
![Databricks logo](slideassets/databricks-white.png)
### Jeff Brockway



## You are here
* Where you are in the Course
* What you've already done
* What you'll walk away with

notes:
#### Synopsis
Help someone familiar with traditional database analytics and reporting understand the need for big data and how the concepts map, with a hands-on example for context.

#### Assumptions
* User has already set up a Databricks account
* User has already created a cluster

#### Primary objective
* Learn big data concepts with Spark and Databricks

#### Learning objectives
* List six use cases for big data
* List four primary tools used for big data analytics
* Map small data concepts and tools to big data
* Describe the three primary Spark dataset types
* Create and navigate a Databricks notebook
* Import a dataset
* Describe transformations and actions
* Use visualizations
* Save and share notebooks
* Identify next steps


<!-- .slide: data-background-image="slideassets/background-gray-dimpled" -->
## Why big data?
Not all jobs can run once daily on a single system



<!-- .slide: data-background="#ff0000" -->
## When would I use it?
1. When it's too big to run on one machine<!-- .element: class="fragment" data-fragment-index="1" -->
2. When you need it faster<!-- .element: class="fragment" data-fragment-index="2" -->



<!-- .slide: data-background="#00ff00" -->
## What are some use cases?
* Cyber security SIEM systems
* Too many events
* Youâ€™re going to need some help



## What tools will I use?
* Apache Spark
* Databricks
* other tools



## How does that compare to what I know?
* SQL DB --> HDFS, DBFS, Delta Lake
* data --> clustered fs (in memory)
* SQL --> Spark
* Shell --> Notebooks
* Apps --> Notebooks
* Code --> Notebooks
* Reporting --> Notebooks

notes:
## Making the leap
Moving from relational database data analytics to big data
like moving from procedure programming to object oriented. It's a big leap.

* data --> clustered fs (in memory)
* SQL --> Spark
* Shell --> Notebooks
* Apps --> Notebooks
* Code --> Notebooks
* Reporting --> Notebooks



## Couldn't I just write a SQL query?
* You can use SQL
* Consider your infrastructure
* How do you scale?



## So what is Apache Spark?



## Ok, so how do I install this on my machine?
* Er, you don't



## Where does Databricks fit in?
* Infra structure
* Clustering and scalability
* Job running
* UI and Notebooks
* Reporting



## How do I get started?
First get some data scientist a on it to explore, and then data engineers to plumb it

* Where is your data now?
* How did it get there?
* How did you transform it?
* What happens when it updates?
* What happens when the schema changes?
* How do you know you are seeing the possibilities?
* Log into Databricks
* Access data sets



# Hands-on practice



## Sign up for Databricks CE
Import a notebook here
Download a PDF here



<!-- .slide: data-background-video="http://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4" -->
## Import a dataset



## View the data



## Perform some actions on it



## Perform some transformations



## Do this with SQL and Python



## Visualize



## Save and share notebook




# Next steps
